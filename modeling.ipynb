{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de57dbbb",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20940c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from constants import models\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['price','year','manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive']\n",
    "df = raw_data[columns]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648f80e",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c55ff",
   "metadata": {},
   "source": [
    "One of the biggest challenges with this dataset was finding the correct make and model of the car. These features will be huge. However because of free text, it was hard to make this happen. (i.e. \"f150\" vs. \"f-150\", or \"rav4\" vs \"rav4 se\"). I decided to go through manually and create a list of the most common models and save them in constants.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]', '', text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text\n",
    "df['cleaned_model'] = df['model'].apply(clean_model)\n",
    "\n",
    "df['base_model'] = 'other' \n",
    "for model in models:\n",
    "    mask = df['cleaned_model'].str.contains(model, case=False, na=False)\n",
    "    df.loc[mask, 'base_model'] = model\n",
    "\n",
    "print(f\"Number of rows with 'other' base_model: {df[df['base_model'] == 'other'].shape[0]}\")\n",
    "print(f\"Dropping these rows. That's {100 * df[df['base_model'] == 'other'].shape[0]/ df.shape[0]:.2f}% rows.\")\n",
    "\n",
    "df = df[df['base_model'] != 'other']\n",
    "df['car_name'] = df['manufacturer'] + ' ' + df['base_model']\n",
    "df.drop(columns=['manufacturer', 'model', 'cleaned_model', 'base_model'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclucating the median and IQR for price and then mileage\n",
    "median_price = df['price'].median()\n",
    "Q1_PRICE = df['price'].quantile(0.25)\n",
    "Q3_PRICE = df['price'].quantile(0.75)\n",
    "IQR_PRICE = Q3_PRICE - Q1_PRICE\n",
    "\n",
    "median_mileage = df['odometer'].median()\n",
    "Q1_MILEAGE = df['odometer'].quantile(0.25)\n",
    "Q3_MILEAGE= df['odometer'].quantile(0.75)\n",
    "IQR_MILEAGE = Q3_MILEAGE - Q1_MILEAGE\n",
    "\n",
    "threshold = 2.5\n",
    "\n",
    "outliers_price = (df['price'] < (Q1_PRICE - threshold * IQR_PRICE)) | (df['price'] > (Q3_PRICE + threshold * IQR_PRICE))\n",
    "outliers_mileage = (df['odometer'] < (Q1_MILEAGE - threshold * IQR_MILEAGE)) | (df['odometer'] > (Q3_MILEAGE + threshold * IQR_MILEAGE))\n",
    "outliers = outliers_price | outliers_mileage\n",
    "df_no_outliers = df[~outliers]\n",
    "\n",
    "# Since both lower bounds are also negative numbers, we will also manually remove all zeros\n",
    "df_no_outliers = df_no_outliers[(df_no_outliers['price'] > 0) & (df_no_outliers['odometer'] > 0)]\n",
    "\n",
    "# As dicussed in exploration notebook, we will manually remove all cars before 2000\n",
    "df_no_outliers = df_no_outliers[df_no_outliers['year'] >= 2000]\n",
    "\n",
    "print(f'Number of outliers removed: {df.shape[0] - df_no_outliers.shape[0]}')\n",
    "print(f'That is {100 * (df.shape[0] - df_no_outliers.shape[0]) / df.shape[0]:.2f}% of the remaining data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932e79b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since cylinders is an ordinal feature, we will convert it to a numerical value\n",
    "# About 100 have \"other\" so we will just remove them\n",
    "df_no_outliers = df_no_outliers[df_no_outliers['cylinders'] != 'other']\n",
    "df_no_outliers['cylinders'] = df_no_outliers['cylinders'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Mapping conditions to numerical values. I checked the value counts and it seems like a good mapping\n",
    "condition_mapping = {'salvage': 1, 'fair': 2, 'good': 3, 'like new': 4, 'excellent': 5, 'new': 6}\n",
    "df_no_outliers['condition'] = df_no_outliers['condition'].map(condition_mapping)\n",
    "\n",
    "# Having a clean title is more important that title status, so will just make this a boolean\n",
    "df_no_outliers['clean_title'] = df_no_outliers['title_status'].apply(lambda x: 1 if x == 'clean' else 0)\n",
    "df_no_outliers = df_no_outliers.drop(columns=['title_status'])\n",
    "\n",
    "# Year is a bad feature, so will turn it into \"Age\"\n",
    "reference_year = 2023\n",
    "df_no_outliers['age'] = reference_year - df_no_outliers['year']\n",
    "df_no_outliers = df_no_outliers.drop(columns=['year'])\n",
    "\n",
    "# One hot encoding all categorical variables\n",
    "encoded_columns = ['car_name','fuel','transmission','drive']\n",
    "df_no_outliers = pd.get_dummies(df_no_outliers, columns=encoded_columns, drop_first=True).astype(int)\n",
    "df_no_outliers.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b43e5",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1bc18",
   "metadata": {},
   "source": [
    "Note: I tried using the log_price, but this actually gave me worse results. I also consistently added and dropped features and tried different models. Most of that isn't shown here, and rather I only kept the end product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c875b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_no_outliers.drop(columns=['price'])\n",
    "y = df_no_outliers['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note I early on decided linear regression wasn't the best model for this task\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ab95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I decided not to drop cylinders since Random Forest handle multicollinearity well\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:,.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:,.2f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920c1ce",
   "metadata": {},
   "source": [
    "Note: Tried Hyperparameter tuning and it gave me worse results. Going to stick with my original model as it is pretty solid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b677309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20, \n",
    "    cv=3, \n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE for minimization\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best CV score:\", np.sqrt(-random_search.best_score_))\n",
    "\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"Tuned RMSE: ${rmse_tuned:,.2f}\")\n",
    "print(f\"Tuned RÂ²: {r2_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b868b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import car_names\n",
    "car_cylinders_mapping = (\n",
    "    df[df['car_name'].isin(car_names)]\n",
    "    .groupby('car_name')['cylinders']\n",
    "    .first()\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = {\n",
    "    'model': model,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'car_cylinders_mapping': car_cylinders_mapping,\n",
    "    'condition_mapping': condition_mapping,\n",
    "    'reference_year': reference_year,\n",
    "    'encoded_columns': encoded_columns,\n",
    "    'version': '1.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Save using gzip compression\n",
    "#with gzip.open('car_price_model_complete.pkl.gz', 'wb') as file:\n",
    "#    pickle.dump(model_package, file)\n",
    "\n",
    "import joblib\n",
    "import bz2\n",
    "\n",
    "with bz2.BZ2File(\"car_price_model_complete.joblib.bz2\", \"wb\") as f:\n",
    "    joblib.dump(model_package, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d96b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('car_price_model_complete.joblib.bz2', 'rb') as file:\n",
    "    model_package = joblib.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ceff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(inputs, model_package):\n",
    "    df = pd.DataFrame([inputs])\n",
    "    df['cylinders'] = df['car_name'].map(model_package['car_cylinders_mapping'])\n",
    "    df['cylinders'] = df['cylinders'].str.extract('(\\d+)').astype(int)\n",
    "    df['condition'] = df['condition'].map(model_package['condition_mapping'])\n",
    "    df['clean_title'] = df['title_status'].apply(lambda x: 1 if x == 'clean' else 0)\n",
    "    df = df.drop(columns=['title_status'])\n",
    "    df['age'] = model_package['reference_year'] - df['year']\n",
    "    df = df.drop(columns=['year'])\n",
    "    df = pd.get_dummies(df, columns=model_package['encoded_columns'], drop_first=True).astype(int)\n",
    "    missing_cols = set(model_package['feature_names']) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        missing_df = pd.DataFrame(0, index=df.index, columns=list(missing_cols))\n",
    "        df = pd.concat([df, missing_df], axis=1)\n",
    "    df = df[model_package['feature_names']]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {\n",
    "    'odometer': 150000,\n",
    "    'car_name': 'dodge charger',\n",
    "    'condition': 'good',\n",
    "    'title_status': 'clean',\n",
    "    'year': 2010,\n",
    "    'fuel': 'gas',\n",
    "    'transmission': 'automatic',\n",
    "    'drive': 'fwd'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa96e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = preprocess_data(test_input, model_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db561564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predictions = np.array([tree.predict(processed_data.values)[0] for tree in model_package['model'].estimators_])\n",
    "point_estimate = np.mean(tree_predictions)\n",
    "std_error = np.std(tree_predictions) / np.sqrt(len(tree_predictions))\n",
    "\n",
    "lower_bound = point_estimate - 1.96 * std_error\n",
    "upper_bound = point_estimate + 1.96 * std_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Point Estimate: ${point_estimate:,.2f}\")\n",
    "print(f\"Upper Bound: ${upper_bound:,.2f})\")\n",
    "print(f\"Lower Bound: ${lower_bound:,.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bd098",
   "metadata": {},
   "source": [
    "# Invesitagtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17527b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transmission'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd778b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package['car_cylinders_mapping'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf921d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['car_name'] == 'ford f150') & (df['year'] == 2014) & (df['price'] != 0)]['price'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6d171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
